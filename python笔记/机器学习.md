# 机器学习

### 简介

* 框架

  * sklearn
  * tensorflow

* 书籍

  * 机器学习
  * TensorFlow技术解析与实战
  * 面向机器智能与TensorFlow实战
  * Python数据分析与挖掘实战
  * 机器学习系统设计

* 机器学习的数据

  * 文件csv
  * mysql
    * 性能瓶颈、读取速度
    * 格式不太符合机器学习要求的数据格式
  * pandas   读取工具
    * 一个数据读取非常方便以及基本的处理格式的工具
  * numpy:     释放GIL锁   多线程
  * sklearn
    * 对于**特征的处理**提供了强大的接口

* 可用的数据集

  * Kaggle
    * 大数据竞赛平台
    * 真实数据
    * 数据量巨大
    * 80万科学家
  * UCI
    * 收集了360个数据集
    * 涵盖科学、生活、经济等领域
    * 数据量几十万
  * scikit-learn
    * 数据量较小
    * 方便学习

* 常用数据集结构

  * 特征值   +   目标值

  

* 特征工程

  * 特征工程是将**原始数据转换为更好地代表预测模型的潜在问题的特征**的过程，从而**提高了对未知数据的预测准确性**

### 数据的特征抽取

* 示例

```
#导入包
from sklearn.feature_extraction.text import CountVectorizer
# 实例化CountVectorizer()
vector = CountVectorizer()
# 调用fit_transform()输入并转换数据
res = vector.fit_transform(['life is short,i like python','life is too short,i dislike python'])
# 打印结果
print(vector.get_feature_names())

print(res.toarray())
```

* 特征抽取对文本等数据进行特征值化
  * 特征值化是为了计算机更好的去理解数据
* 特征抽取API
  * sklearn.feature_extraction

#### 字典特征抽取

* 作用：对字典数据进行特征值化
* 类：
  * sklearn.feature_extraction.DictVectorizer
* 安装sklearn库
  * <http://www.lfd.uci.edu/~gohlke/pythonlibs/> 

##### DictVectorizer语法

* DictVectorizer(sparse = True,...)
  * DictVectorizer.fit_transform(X)
    * X：字典或者是包括字典的迭代器
    * 返回值：返回sparse矩阵
  * DictVectorizer.inverse_transform(X)
    * X：array数组或者sparse矩阵
    * 返回值：转换之前的数据格式
  * DictVectorizer.get_feature_names()
    * 返回类别名称
  * DictVectorizer.transform(X)
    * 按照原先的标准转换
* 流程
  * 1、实例化类DictVectorizer
  * 2、调用fit_transform方法输入数据并转换

#### 文本特征抽取

* 作用：

  * 对文本数据进行特征值化

* 类：

  * sklearn.feature_extraction.text.CountVectorizer

* CountVectorizer()

  * 返回词频矩阵
  * CountVectorizer.fit_transform(X)
    * X: 文本或者文本字符串的可迭代对象
    * 返回值：返回sparse矩阵
  * CountVectorizer.inverse_transform(X)
    * X:array数组或者sparse矩阵
    * 返回值：转换之前的格式
  * CountVectorizer.get_feature_names()
    * 返回值：单词列表

* 单个英文字母不统计：

  > 因为单个英文字母不能代表情感等因素、没有分类依据

* 流程

  * 实例化类  CountVectorizer
  * 调用fit_transform方法输入数据并转换
    * 注意返回格式。利用toarray()进行sparse矩阵转换array数组

* 中文文本特征分析

  * 使用**jieba**进行分词

    ```
    import jieba
    jieba.cut("人生苦短我用python")
    ```

    * 返回值：词语生成器、

```
def cutChinese():
    """
    中文特征值化，切割中文文本
    :return:
    """
    con1 = jieba.cut("今天很残酷，明天更残酷，后天很美好，但绝大多数的人是死在了明天的晚上，所以每个人不要放弃今天")
    con2 = jieba.cut("当我们看到从很远的星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看他的过去")
    con3 = jieba.cut("白日依山尽，黄河入海流")
    # 转换成列表
    content1 = list(con1)
    content2 = list(con2)
    content3 = list(con3)

    # 把列表转换成字符串
    c1 = ' '.join(content1)
    c2 = ' '.join(content2)
    c3 = ' '.join(content3)
    return c1, c2, c3

def featureChinese():
    """
    中文特征值化
    :return:
    """
    c1, c2, c3 = cutChinese()
    print(c1, c2, c3)
    cv = CountVectorizer()
    data = cv.fit_transform([c1, c2, c3])
    print(cv.get_feature_names())
    print(data.toarray())

```

* 流程：
  * 准备文本，利用jieba进行分词
  * 实例化CountVectorizer
  * 将分词结果变成字符串当作fit_transform的输入值

#### tf-idf

* 简介：
  * Tf: term frequency:词的频率    出现的次数
  * idf:逆文档频率  inverse document trequency
    * log(总文档数量/该词出现的文档数量)
    * 如果词出现在很多篇文章，log(总文档数量/该词出现的文档数量)值就小，表示词重要性就小，如“明天”这样的词
    * log(总文档数量/该词出现的文档数量)大，表示重要程度大
  * tf * idf 
    * 重要性程度
  * TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率高，并且在其他的文章中很少出现，则认为此词或短语具有很好的类别区分能力，适合用来分类
  * TF-IDF作用：用以评估一个词对于一个文件集或一个语料集中的其中一份的重要程度
  * 类：sklearn.feature_extraction.text.TfidfVectorizer
* TfidfVectorizer语法
  * TfidfVectorizer（stop_words = None,...）
    * 返回次的权重矩阵
    * TfidfVectorizer.fit_transform(X)
      * X:文本或者包含文本字符串的可迭代对象
      * 返回值：返回sparse矩阵
    * TfidfVectorizer.inverse_transform(X)
      * X:array数组或者sparse矩阵
      * 返回值：转换之前的数据形式
    * TfidfVectorizer.get_feature_names
      * 返回值：单词列表

### 数据特征预处理

> 通过特定的统计方法（数学方法），将数据转换成**算法要求**的数据
>
> **数值型数据**

* 数值型数据
  * 标准缩放
    * 归一化
    * 标准化
    * 缺失值处理
  * 类别型数据：one-hot编码
  * 时间类型：时间的切分
* API：
  * sklearn.preprocessing

#### 归一化

* 特点：通过对原始数据进行变换把数据映射到(默认为[0,1])之间

* 公式： 
  $$
  \chi’ = \frac{\chi - min}{max-min}
  $$



$$
\chi'' = \chi' * (mx-mi )+mi
$$

* 注：作用于每一列，max为一列最大值，min为一列最小值，那么x''为最终结果，mx,mi分别为指定区间值默认mx为1，mi为0
* sklearb归一化API
  * sklearn归一化API:sklearn.preprocessing.MinMaxScaler
* MinMaxScaler语法
  * MinMaxScaler(feature_range = (0,1)...)
    * 每个特征缩放到给定范围（默认[0,1]）
    * MinMaxScaler.fit_transform(X)
      * X:numpy array格式的数据[n_samples, n_features]
      * 返回值：转换后的形状相同的array
  * 步骤
    * 实例化MinMaxScaler
    * 调用fit_transform(X)
* 目的：
  * 使得某一特征对最终结果不会造成更大的影响
* 注意在特定场景下最大值与最小值是变化的，另外，最大值与最小值非常容易受**异常点**影响，所以这种方法**鲁棒性**较差，只适合传统精确小数据场景

#### 标准化

* 公式

* $$
  X'=\frac{x-mean}{\delta}
  $$

  >  **作用于每一列**，mean为平均值，δ 为标准差，var为方差
  > $$
  > var = \frac{(x1-mean)^2+(x2-mean)^2+...}{n(每个特征的样本数)}
  > $$
  >
  > $$
  > δ  =\sqrt(var)
  > $$
  >
  > 其中，方差考量数据的稳定性

* 结合归一化来谈标准化

  * 对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变
  * 对于标准化来说：如果出现异常点，由于具有一定的数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小

* 标准差API

  * sklearn特征话API：scikit-learn.preprocessing.StandardScaler

* StandardScaler语法

  * StandardScaler(...)
    * 处理之后每列来说所有数据都聚集在**均值0方差为1附近**
    * StandardScaler.**fit_transform(X)**
      * X:numpy  array格式的数据[n_sample, n_features]
      * 返回值：转换之后的形状相同的array
    * StandardScaler.mean_
      * 原始数据中每列特征的平均值
    * StandardScaler.std_
      * 原始数据每列特征的方差
  * 标准化步骤
    * 实例化StandardScaler
    * 通过fit_transform转换

* 标准化总结

  * 适合嘈杂大数据场景

#### 缺失值

* 用pandas工具
* 删除：如果每列或每行数据缺失值达到一定的比例，建议放弃整行或整列
* **插补**：可以通过缺失值每行或每列的平均值、中位数来填充
* sklearn缺失值API：sklearn.preprocessing.Imputer
* **Imputer语法**
  * Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
    * 完成缺失值填补
    * axis=0,表示列，axis=1表示行
    * Imputer.**fit_transform(X)**
      * X:numpy array格式的数据[n_samples, n_features]
      * 返回值：转换后的形状相同的array
    * 数据中的缺失值：np.nan/np.NaN